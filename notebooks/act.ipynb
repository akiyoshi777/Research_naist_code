{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/is/akiyoshi-n/my-project\n"]}],"source":["cd .."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10379,"status":"ok","timestamp":1687230849685,"user":{"displayName":"Yuka Otsuki","userId":"18027213593861968720"},"user_tz":-540},"id":"XFCiFuR5e9M_"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import time\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","from torch import cuda\n","from src.my_project.dataset import load_dataset, load_text_dataset\n","from src.my_project.train import ActClassifier\n","from transformers import BertTokenizer, BertJapaneseTokenizer, BertForSequenceClassification\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from src.my_project.dataset import preprocess_dataset, preprocess_multiclass_dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1687230849686,"user":{"displayName":"Yuka Otsuki","userId":"18027213593861968720"},"user_tz":-540},"id":"oxcz0wrxfDNQ"},"outputs":[],"source":["DATASET_PATH = Path('/home/is/akiyoshi-n/my-project/data')\n","output_dir = Path('/home/is/akiyoshi-n/my-project/outputs')\n","# パラメータの設定\n","MAX_LEN = 128 # 最大トークン数．本データでは最大トークン数が105なので，全て符号化できる．\n","# DROP_RATE = 0.4\n","BATCH_SIZE = 16 # バッチサイズ：分割した時の一つのデータ数．一回で学習するデータ数\n","NUM_EPOCHS = 100 # 何周学習させるか．エポック数\n","LEARNING_RATE = 2e-5 # 学習率\n","num_folds = 5\n","PATIENCE = 5 # 忍耐値．早期停止の役割を担う．モデル性能が改善されないエポック数が5を超えると訓練停止\n","SEED = 2023 # 乱数シード\n","num_labels = 2\n","\n","# デバイスの指定\n","device = \"cuda:1\" if cuda.is_available() else \"cpu\"\n","\n","timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/is/akiyoshi-n/my-project/.venv/lib/python3.11/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n","  warn(msg)\n"]}],"source":["# データの読み込み\n","data = load_dataset(f\"{DATASET_PATH}/act_classification_final.xlsx\")\n","# 損失関数の定義\n","CRITERION = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228434,"status":"ok","timestamp":1687231078098,"user":{"displayName":"Yuka Otsuki","userId":"18027213593861968720"},"user_tz":-540},"id":"Y0yjm7N_hxsv","outputId":"fc01951e-a405-4524-f523-48cc0f57e679"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------Fold: 1-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.5863636363636363\n","macro f1: 0.5565989679076875\n","-----------------Fold: 2-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6363636363636364\n","macro f1: 0.6339129711290457\n","-----------------Fold: 3-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6681818181818182\n","macro f1: 0.665130632415188\n","-----------------Fold: 4-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6727272727272727\n","macro f1: 0.6699999999999999\n","-----------------Fold: 5-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6090909090909091\n","macro f1: 0.6064564439637241\n","-------------------------------------\n","finished epochs : [28, 31, 23, 24, 31]\n","Average Accuracy: 0.6345454545454545\n","Average macro f1: 0.626419803083129\n"]}],"source":["# 事前学習済みモデル（BERT）の指定\n","MODEL_NAME = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 交差検証の実行\n","trainer = ActClassifier(tokenizer=tokenizer, model_name = MODEL_NAME, criterion=CRITERION, device=device, seed=SEED)\n","fold_scores, fold_f1, best_model = trainer.cross_validation(MODEL_NAME, data, num_folds, NUM_EPOCHS, \n","                                                            device, LEARNING_RATE, \n","                                                            BATCH_SIZE, output_dir, MAX_LEN,timestamp)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------Fold: 1-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7045454545454546\n","macro f1: 0.704246034208186\n","-----------------Fold: 2-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7\n","macro f1: 0.6975\n","-----------------Fold: 3-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7363636363636363\n","macro f1: 0.7263916287846299\n","-----------------Fold: 4-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.759090909090909\n","macro f1: 0.7554068511254222\n","-----------------Fold: 5-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6409090909090909\n","macro f1: 0.638752052545156\n","-------------------------------------\n","finished epochs : [10, 12, 19, 13, 18]\n","Average Accuracy: 0.708181818181818\n","Average macro f1: 0.7044593133326789\n"]}],"source":["# 東北大BERT-v2\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v2'\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","trainer = ActClassifier(tokenizer=tokenizer, model_name = MODEL_NAME, criterion=CRITERION, device=device, seed=SEED)\n","fold_scores, fold_f1, best_model, all_predictions, all_labels = trainer.cross_validation(MODEL_NAME, data, num_folds, NUM_EPOCHS, \n","                                                            device, LEARNING_RATE, \n","                                                            BATCH_SIZE, output_dir, MAX_LEN, timestamp, num_labels)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 0, 1, 1, 1, 0, 0, 0, 1, 1]\n","[1, 0, 1, 1, 1, 0, 0, 0, 1, 1]\n"]}],"source":["print(all_predictions[:10])\n","print(all_labels[:10])"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32768, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["best_model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32768, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# 保存したモデルのディレクトリパス\n","model_path = f\"{output_dir}/cl-tohoku/bert-base-japanese-v22023-11-21T20:15:32\"\n","\n","# モデルを読み込む\n","model = BertForSequenceClassification.from_pretrained(model_path)\n","\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------Fold: 1-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Downloading (…)lve/main/config.json: 100%|██████████| 473/473 [00:00<00:00, 917kB/s]\n","Downloading pytorch_model.bin: 100%|██████████| 1.35G/1.35G [01:58<00:00, 11.4MB/s]\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7318181818181818\n","macro f1: 0.729802868502675\n","-----------------Fold: 2-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6772727272727272\n","macro f1: 0.6644540396554317\n","-----------------Fold: 3-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7090909090909091\n","macro f1: 0.7055871528939445\n","-----------------Fold: 4-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7272727272727273\n","macro f1: 0.724517906336088\n","-----------------Fold: 5-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-large-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6636363636363637\n","macro f1: 0.6626326259946951\n","-------------------------------------\n","finished epochs : [48, 11, 12, 14, 12]\n","Average Accuracy: 0.7018181818181819\n","Average macro f1: 0.6973989186765668\n"]}],"source":["'''\n","# 東北大BERT-v2\n","MODEL_NAME = 'cl-tohoku/bert-large-japanese-v2'\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","trainer = ActClassifier(tokenizer=tokenizer, model_name = MODEL_NAME, criterion=CRITERION, device=device, seed=SEED)\n","fold_scores, fold_f1, best_model = trainer.cross_validation(MODEL_NAME, data, num_folds, NUM_EPOCHS, \n","                                                            device, LEARNING_RATE, \n","                                                            BATCH_SIZE, output_dir, MAX_LEN, timestamp)\n","'''"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"ActClassifier.cross_validation() missing 1 required positional argument: 'num_labels'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/is/akiyoshi-n/my-project/notebooks/act.ipynb セル 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# 交差検証の実行\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m trainer \u001b[39m=\u001b[39m ActClassifier(tokenizer\u001b[39m=\u001b[39mtokenizer, model_name \u001b[39m=\u001b[39m MODEL_NAME, criterion\u001b[39m=\u001b[39mCRITERION, device\u001b[39m=\u001b[39mdevice, seed\u001b[39m=\u001b[39mSEED)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m fold_scores, fold_f1, best_model \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mcross_validation(MODEL_NAME, data, num_folds, NUM_EPOCHS, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                                                             device, LEARNING_RATE, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m                                                             BATCH_SIZE, output_dir, MAX_LEN, timestamp)\n","\u001b[0;31mTypeError\u001b[0m: ActClassifier.cross_validation() missing 1 required positional argument: 'num_labels'"]}],"source":["# 東北大BERT-v3\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 交差検証の実行\n","trainer = ActClassifier(tokenizer=tokenizer, model_name = MODEL_NAME, criterion=CRITERION, device=device, seed=SEED)\n","fold_scores, fold_f1, best_model = trainer.cross_validation(MODEL_NAME, data, num_folds, NUM_EPOCHS, \n","                                                            device, LEARNING_RATE, \n","                                                            BATCH_SIZE, output_dir, MAX_LEN, timestamp)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","-------------------------------------\n","finished epochs : 13\n"]}],"source":["# 東北大BERT-v2，1100件全部のデータを用いて学習を行う．\n","# 東北大BERT-v2\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v2'\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","trainer = ActClassifier(tokenizer=tokenizer, model_name = MODEL_NAME, criterion=CRITERION, device=device, seed=SEED)\n","model = trainer.train_model_all_data(MODEL_NAME, data, NUM_EPOCHS, device, LEARNING_RATE, BATCH_SIZE, output_dir, MAX_LEN, timestamp, num_labels)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["'''\n","import pickle\n","# インスタンスをファイルに保存\n","save_path = f'{output_dir}/all_data_ActClassifier_tohoku_v2.pkl'\n","with open(save_path, 'wb') as output_file:\n","    pickle.dump(trainer, output_file)\n","'''"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# 保存したインスタンスを読み取り\n","import pickle\n","load_path = f'{output_dir}/all_data_ActClassifier_tohoku_v2.pkl'\n","with open(load_path, 'rb') as input_file:\n","    trainer = pickle.load(input_file)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# 半教師あり学習を用いるためにmodelを用いて各テキストの確率を算出\n","# まずは，テストデータの読み込み\n","test_data = load_text_dataset('/home/is/akiyoshi-n/my-project/notebooks/data_extraction/add_data_sub.txt.xlsx')"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["probabilities, labels = trainer.prediction(test_data)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["add_data = trainer.get_text_and_label(test_data=test_data, total_probabilities=probabilities, total_labels=labels, threshold=0.99)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["add_data1 = add_data['texts']\n","add_data2 = add_data['texts']\n","add_data3 = add_data1 + add_data2"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["Counter({1: 2243, 0: 1069})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from collections import Counter\n","Counter(add_data['labels'])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'list' object has no attribute 'value_counts'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/home/is/akiyoshi-n/my-project/notebooks/act.ipynb セル 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# リストのadd_data['labels']の値を確認\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bkogecha/home/is/akiyoshi-n/my-project/notebooks/act.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m add_data[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalue_counts()\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'value_counts'"]}],"source":["# リストのadd_data['labels']の値の個数を確認\n","add_data['labels']"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------Fold: 1-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7090909090909091\n","macro f1: 0.7049702455787444\n","-----------------Fold: 2-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.75\n","macro f1: 0.7435082140964494\n","-----------------Fold: 3-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.740909090909091\n","macro f1: 0.7261948950850455\n","-----------------Fold: 4-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.7136363636363636\n","macro f1: 0.7092572003943698\n","-----------------Fold: 5-----------------\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Early stopping. No improvement in loss.\n","Accuracy: 0.6727272727272727\n","macro f1: 0.67098703888335\n","-------------------------------------\n","finished epochs : [13, 10, 15, 20, 9]\n","Average Accuracy: 0.7172727272727274\n","Average macro f1: 0.7109835188075918\n"]}],"source":["# 東北大BERT-v2\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-v2'\n","fold_scores, fold_f1, best_model, all_predictions, all_labels = trainer.Semi_Supervised_Learning_cross_validation(MODEL_NAME, data, add_data, num_folds, NUM_EPOCHS, \n","                                                            device, LEARNING_RATE, \n","                                                            BATCH_SIZE, output_dir, MAX_LEN, timestamp, num_labels)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM95jTX+4zQTo8ar6RmLV6c","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
