{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは半教師あり学習を用いた分類を行っている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/is/akiyoshi-n/my-project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 使用するGPUを指定. この環境変数の場所は，pytorchをimportする前に入れる\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from src.my_project.dataset import load_dataset_2class_classification, split_test_data_stratify, load_text_dataset, preprocess_for_Trainer\n",
    "from src.my_project.train_v2 import ActClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = Path('/home/is/akiyoshi-n/my-project/data')\n",
    "# 本日の日付\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# 出力先ディレクトリ\n",
    "output_dir = Path('/home/is/akiyoshi-n/my-project/outputs/{}'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大トークン数\n",
    "MAX_LEN = 128\n",
    "# バッチサイズ\n",
    "BATCH_SIZE = 16\n",
    "# エポック数\n",
    "NUM_EPOCHS = 100\n",
    "# 学習率\n",
    "LEARNING_RATE = 2e-5\n",
    "# Cross Validation時のFold数\n",
    "NUM_FOLDS = 3\n",
    "# 早期停止のための忍耐値\n",
    "PATIENCE = 2\n",
    "# 乱数シード\n",
    "SEED = 2023\n",
    "# クラス数\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書型でデータ取得\n",
    "data = load_dataset_2class_classification(f\"{DATASET_PATH}/act_classification_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 東北大BERT-v3\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'\n",
    "Classifier_model1 = ActClassifier(model_name = MODEL_NAME, num_labels=NUM_LABELS, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータと訓練に使用するデータに分割\n",
    "train_dataset, test_dataset, train_indices, test_indices = split_test_data_stratify(data=data, test_size=0.1, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割\n",
    "train_dataset, eval_dataset, train_indices, eval_indices = split_test_data_stratify(data=train_dataset, test_size=0.2, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 198, 110)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['labels']), len(eval_dataset['labels']), len(test_dataset['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Parameter 'fn_kwargs'={'tokenizer': BertJapaneseTokenizer(name_or_path='cl-tohoku/bert-base-japanese-v3', vocab_size=32768, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, 'max_len': 128} of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ede2c7a9a84678be22958cf2635ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d569f4c08f48178cefea76ecf9900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 200/5000 00:41 < 16:54, 4.73 it/s, Epoch 4/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.698593</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651700</td>\n",
       "      <td>0.656092</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>0.634361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.659394</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.619289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>0.822173</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.645455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = Classifier_model1.train_model(train_dataset, eval_dataset, MAX_LEN, NUM_EPOCHS, LEARNING_RATE, BATCH_SIZE, PATIENCE, output_dir, project_name='ActClassification_2class', run_name='no_semi_supervised_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b212ff4c10c4bbbb758fc950269034f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model1を用いてtestデータの予測を行う\n",
    "# tokenizerの定義\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "test_dataset = preprocess_for_Trainer(test_dataset, tokenizer, MAX_LEN)\n",
    "pred_logits_test = model1.predict(test_dataset).predictions\n",
    "pred_test = np.argmax(pred_logits_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5909\n",
      "F1: 0.6400\n"
     ]
    }
   ],
   "source": [
    "# テストデータで評価\n",
    "true_test = test_dataset['labels']\n",
    "accuracy = accuracy_score(true_test, pred_test)\n",
    "f1 = f1_score(true_test, pred_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = load_text_dataset(f\"{DATASET_PATH}/add_data_sub.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data['texts'] = set(add_data['texts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6768"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(add_data['texts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 半教師あり学習に使用する既存のモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータと訓練に使用するデータに分割\n",
    "train_dataset, eval_dataset, train_indices, eval_indices = split_test_data_stratify(data=data, test_size=0.1, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 東北大BERT-v3\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'\n",
    "Classifier_model2 = ActClassifier(model_name = MODEL_NAME, num_labels=NUM_LABELS, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc19c2b19d2340ff882b5905d2b74e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067e0ec94897411a8733d887a291fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51db3159f6440f9a401a1de67826bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111216853476233, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='6200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 310/6200 00:59 < 19:06, 5.14 it/s, Epoch 5/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.680416</td>\n",
       "      <td>0.572727</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.616200</td>\n",
       "      <td>0.662545</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.556701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.510400</td>\n",
       "      <td>0.652863</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.343600</td>\n",
       "      <td>0.720981</td>\n",
       "      <td>0.690909</td>\n",
       "      <td>0.595238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>1.141681</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.513514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = Classifier_model2.train_model(train_dataset, eval_dataset, MAX_LEN, NUM_EPOCHS, LEARNING_RATE, BATCH_SIZE, PATIENCE, output_dir, project_name='ActClassification_2class', run_name='base_for_semi_supervised_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256fee9ef1f40f49d0bd095ccf12ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add_dataをTrainerに入力できる形式に変更\n",
    "# tokenizerの定義\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "add_data = preprocess_for_Trainer(add_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logits値を予測\n",
    "logits = model2.predict(add_data)\n",
    "# sigmoid関数を適応できるようにTensorに変換\n",
    "logits = torch.from_numpy(logits.predictions)\n",
    "# シグモイド関数を適用し，確率に変換\n",
    "predictions_proba = torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大値が0.7以上の行をindexと共に取得\n",
    "indices = torch.where(predictions_proba.max(dim=1).values > 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数の値が入ったリストのindices[0]の行を取得\n",
    "use_data = {'texts':[add_data['texts'][i] for i in indices[0]],\n",
    "            'labels': np.argmax(predictions_proba[indices[0]], axis=-1).tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(use_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use_dataをtrain_datasetに加えて半教師あり学習を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 東北大BERT-v3\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-v3'\n",
    "Classifier_model3 = ActClassifier(model_name = MODEL_NAME, num_labels=NUM_LABELS, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータと訓練に使用するデータに分割\n",
    "train_dataset, test_dataset, train_indices, test_indices = split_test_data_stratify(data=data, test_size=0.1, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割\n",
    "train_dataset, eval_dataset, train_indices, eval_indices = split_test_data_stratify(data=train_dataset, test_size=0.2, SEED=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v3 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005e362f63cf49a08c737c4e2724e6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd91b1dfb544eb1b472133243c4a903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6aa09f0dc74c44a387820f5f05d888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112267130778895, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 180/6000 00:30 < 16:29, 5.88 it/s, Epoch 3/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679300</td>\n",
       "      <td>0.696834</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>0.587629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.544600</td>\n",
       "      <td>0.715347</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>0.558659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.786132</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.560976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model3 = Classifier_model3.train_model_adding_data(train_dataset, eval_dataset, use_data, MAX_LEN, NUM_EPOCHS, LEARNING_RATE, BATCH_SIZE, PATIENCE, output_dir, project_name='ActClassification_2class', run_name='semi_supervised_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cc979fecc244f3a9734fd102ebd4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model3を用いてtestデータの予測を行う\n",
    "# tokenizerの定義\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "test_dataset = preprocess_for_Trainer(test_dataset, tokenizer, MAX_LEN)\n",
    "pred_logits_test = model3.predict(test_dataset).predictions\n",
    "pred_test = np.argmax(pred_logits_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5091\n",
      "F1: 0.4600\n"
     ]
    }
   ],
   "source": [
    "# テストデータで評価\n",
    "true_test = test_dataset['labels']\n",
    "accuracy = accuracy_score(true_test, pred_test)\n",
    "f1 = f1_score(true_test, pred_test)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
